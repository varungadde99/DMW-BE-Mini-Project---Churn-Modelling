# -*- coding: utf-8 -*-
"""DMW PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sawk4EHm5hJ4afKKqvUWA3EsZIZuSryq

#DMW MINI PROJECT
> Varun Gadde BECOB203 <br>
> Vedant Upganlawar BECOB270 
## BANK CUSTOMERS EXITNG PREDICTIVE MODEL
###A bank is investigating a very high rate of customer leaving the bank. Here is a 10,000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.

###IPYNB FLOW
* [i. Loading Libraries](#A) <br>
* [ii. Importing data](#B) <br>
* [1.Dropping unnecessary columns](#1) <br>
* [2.Variable Analysis](#2) <br>
    * [2.1. Categorical Variables](#2.1) <br>
    * [2.2. Numerical Variables](#2.1) <br>
* [3.Machine Learning](#3) <br>
    * [3.1. Modelling](#3.1) <br>
    * [3.2. Logistic Regression](#3.2) <br>
    * [3.3. Classification with KNN (K Neirest Neighbour)](#3.3) <br>
    * [3.4. Classification with SVM (Support Vector Machine)](#3.4) <br>
    * [3.5. Classification with Naive Bayes](#3.5) <br>
    * [3.6. Classification with Random Forest](#3.6) <br>
    * [3.7. K-Fold Cross Validation (cross_val_score)](#3.7) <br>

<a id="A"></a>
## i. Loading Libraries
"""

# This Python 3 environment comes with many helpful analytics libraries installed
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

"""<a id="B"></a>
## ii. Importing data
"""

# Read data
!git clone https://github.com/varungadde99/DMW-Mini-Project-Classification-SPPU.git

# Commented out IPython magic to ensure Python compatibility.
# %cd DMW-Mini-Project-Classification-SPPU
data = pd.read_csv("Churn_Modelling.csv")

# First 10 rows of data
data.head(10)

# Statistical infos on data
data.describe()

data.info()

"""* From above, we understood that there is no missing value in columns.
* There are 10000 items and each columns has 10000 non-null items.
* We can check missing value also by isnull() method as below:
"""

data.isnull().any()

"""<a id="1"></a>
## 1. Dropping unnecessary columns
"""

data.drop(["RowNumber","CustomerId","Surname"], axis=1, inplace=True)
data.head()

"""<a id="2"></a>
# 2.VARIABLE ANALYSIS

### Categorical Variables:
* Geography
* Gender	
* HasCrCard	
* IsActiveMember	
* Exited
* NumOfProducts

### Numerical Variables: 
* CreditScore	
* Age	
* Tenure	
* Balance	
* EstimatedSalary

<a id="5"></a>
# 3. MACHINE LEARNING
"""

#definign a function that normalizes selected feature of a selected data

def normalize_feature(variable, data1):
    data1[variable] = (data1[variable] - np.min(data1[variable]))/(np.max(data1[variable] - np.min(data1[variable])))

# Normalizing some features in a for loop:
#for each in ['CreditScore','Age', 'Tenure', 'Balance','EstimatedSalary']:
    #normalize_feature(each, new_data)

"""## Converting "Geography" and "Gender" to categoricals:
* Geography --> Geography_France  / Geography_Germany / Geography_Spain
* Gender --> Gender_Female  / Gender_Male

"""

df = data.copy()

df["Geography"] = df.Geography.astype("category")
df["Gender"] = df.Gender.astype("category")
df = pd.get_dummies(df,columns=["Geography","Gender"])

df.head()

"""## Converting "Tenure" and "NumOfProducts" to categorical"""

df["Tenure"] = df.Tenure.astype("category")
df["NumOfProducts"] = df.NumOfProducts.astype("category")
df = pd.get_dummies(df, columns=(["Tenure","NumOfProducts"]))

"""## Converting data types of columns to category"""

for each in ["HasCrCard","IsActiveMember","Exited"]:
    df[each] = df[each].astype("category")

df.info()

df.head(8)

"""<a id="5.1"></a>
## 3.1. Modelling
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

"""## Train - test split

### Creating X_train:
"""

#creating X_train and y_train 
X_train = df.drop("Exited", axis = 1)
y_train = df["Exited"]

#Features (CreditScore, Age, Balance, EstimatedSalary) to be normalized:
for each in ["CreditScore", "Age", "Balance", "EstimatedSalary"]:
    normalize_feature(each, X_train)

#X_train and Y_train data shape summary
print("Length of X_train: ",len(X_train))
print("Shape of X_train: ", X_train.shape)
print("Length of Y_tain: ", len(y_train))
print("Shape of Y_train: ", y_train.shape)

"""* Features (CreditScore, Age, Balance, EstimatedSalary) to be normalized:"""

X_train.describe()

"""### Creating y_train:

### Splitting X_Train, y_train
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_train, 
    y_train, 
    test_size = 0.33, 
    random_state = 42
)

print("Length of X_train: ",len(X_train))
print("Length of X_test: ",len(X_test))
print("Length of y_train: ",len(y_train))
print("Length of y_test: ",len(y_test))

"""![image.png](attachment:image.png)

<a id="5.2"></a>
## 3.2. Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

lr = LogisticRegression(random_state = 1)
lr.fit(X_train, y_train)
y_pred=lr.predict(X_test)

print("Accuracy with test data: ",round(lr.score(X_test, y_test)*100))
cm=confusion_matrix(y_test,y_pred)
print()
print(cm)
print()
print(classification_report(y_test, y_pred))

"""<a id="5.3"></a>
## 3.3. Classification with KNN (K Neirest Neighbour)

### Calculation for single n-neighbor
"""

# Single run for 4-neighbors:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X_train, y_train)
y_pred=knn.predict(X_test)

print("4-neighbors KNN accuracy with test data: ", round(knn.score(X_test,y_test)*100))
cm=confusion_matrix(y_test,y_pred)
print()
print(cm)
print()
print(classification_report(y_test, y_pred))

"""<a id="5.4"></a>
## 3.4. Classification with SVM (Support Vector Machine)
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report


svm = SVC(random_state = 1)
svm.fit(X_train, y_train)
y_pred=svm.predict(X_test)

print("SVM accuracy with test data :", svm.score(X_test, y_test)*100)
cm=confusion_matrix(y_test,y_pred)
print()
print(cm)
print()
print(classification_report(y_test, y_pred))

"""<a id="5.5"></a>
## 3.5. Classification with Naive Bayes
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report


nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred=nb.predict(X_test)

print("NB accuracy with test data :", nb.score(X_test, y_test)*100)
cm=confusion_matrix(y_test,y_pred)
print()
print(cm)
print()
print(classification_report(y_test, y_pred))

"""<a id="5.7"></a>
## 3.6. Classification with Random Forest

### RF classification with single n-estimator
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

rf = RandomForestClassifier(n_estimators = 24, random_state = 42)
rf.fit(X_train, y_train)
y_pred=rf.predict(X_test)

print("RF accuracy with test data :", rf.score(X_test, y_test)*100)
cm=confusion_matrix(y_test,y_pred)
print()
print(cm)
print()
print(classification_report(y_test, y_pred))

"""## <a id="5.8"></a>
## 3.7. K-Fold Cross Validation (cross_val_score)

#### accuracies = cross_val_score(estimator = knn, X = x_train, y=y_train, cv=10)

* Algorithm:
1.     1- Split data as train and test datas.
1.     2- Let say k of k-fold is 3, then train data to be splitted into 3.
1.     3- Split each train data into 3 again as train-train-validation.
1.     4- For each 3 splits, data to be trained and validated.
1.     5- There will be 3 accuracy rate for each 3 trains.
1.     6- Get mean of 3 accuracies.
1.     7- Test the model with test split.
"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator =knn, X = X_train, y = y_train, cv = 10)
print("accuracies :",accuracies)
print("mean accuracy of KNN:", accuracies.mean()*100)